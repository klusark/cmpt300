\documentclass{report}
\usepackage[vmargin=2.5cm,hmargin=2.5cm]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{color}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\dsum}{\displaystyle\sum}
\def\thesection{\arabic{section}}
\begin{document}
%Set definitions for listings
\lstset{ %
language=C++,                     % the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=2,                   % the step between two line-numbers. If it's 1, each line 
                                % will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,          % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                   % adds a frame around the code
tabsize=2,                      % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
title=\lstname,                 % show the filename of files included with \lstinputlisting;
                                % also try caption instead of title
escapeinside={\%*}{*)},         % if you want to add a comment within your code
morekeywords={*,...}            % if you want to add more keywords to the set
}

\input{./Title.tex}
\section{Introduction}
Monitors are a powerful technique for providing mutual exclusion to shared
resources in a multiprogramming environment, because of their ability to simplify the
programming interface for users of the shared resource. They are, however, a complex
construct that must be supported by the compiler for the language that makes use of them.
Seeing as the design goals of C/C++ are primarily based on speed and efficiency of the
resulting code, Monitors are not supported in these languages. 

In this report we design and implement a software interface that simulates the interface
of a Monitor, using the pthreads package to provide the mutual exclusion required. We then
use the Monitor construct to provide mutual exclusion to a hard drive simulator, which receives IO requests 
from multiple users.

\section{High-level Design} %Joel sucks
\subsection{Monitor Design}
Since the monitor construct has to be implemented as a language feature, we can only hope to
simulate the interface of a monitor in C++. The strategy is to use object oriented design
and write a Monitor class which has the same interface as a rudimentary monitor, along
with some extra helper functions to provide the functionality which would typically be
language features. 

Once the Monitor class has been written, a programmer would then be able to write their
own monitor as a subclass of the Monitor class, and implement their monitor functions
according the documentation provided with the Monitor class. As an example, in this
report, we implement a hard drive simulation, and provide mutual exclusion to the shared
resources (IO read/write head) by writing a subclass of the Monitor called HDMonitor.
Figure \ref{fig:UML} features a UML class diagram of the intended system.
\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.5]{300UML.pdf}
    \label{fig:UML}
    \caption{UML diagram from the hard drive simulation.}
\end{figure}

As is see in the diagram, the Monitor class has a very basic interface, with only a few
function calls, most of which are typical monitor calls like \emph{signal} and
\emph{wait}. Two functions which are not standard are \emph{EnterMonitor} and
\emph{LeaveMonitor}. The monitor construct is supposed to only allow one process to use
one of its function calls at a time, and the mutual exclusion is supposed to be provided
automatically by the compiler. This is not possible using C++, and so when writing a
monitor function as a subclass of the Monitor class, the programmer must ensure that the
function calls EnterMonitor before doing anything, and calls LeaveMonitor when the
function's work is done. The public interface and implementation in C++ can be found in
listings \ref{listing:monitor_h} and \ref{listing:monitor_cpp}.

\subsection{Hard Drive Simulator Design}
To demonstrate both the correctness of the Monitor class and how to implement a monitor as
a subclass of Monitor, we implement a hard disk simulation in C++ using the Monitor
interface to provide mutual exclusion to the shared resource (the IO read/write head).
All of the source code can be found in the Listings section.
Multiple threads will be generated to request IO on the tracks of the hard disk (labelled
1 to 15 for our experiments), and the hard disk will be responsible for managing these
requests and servicing them according to a scheduling algorithm. The basic picture is
illustrated in figure \ref{fig:sched}. In our design, the hard disk simulator will read
requests from standard input, which can be generated by any number of threads.
\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.6]{Scheduling_Env.png}
    \label{fig:sched}
    \caption{Scheduling environment for the hard drive simulation.}
\end{figure}

We will also take this opportunity to compare the performance of two popular scheduling
algorithms for real hardware, namely the Elevator algorithm and the Shortest Seek-Time
First (SSTF) algorithm. In the Elevator algorithm, the IO head moves in one direction,
servicing pending requests as it reaches the desired tracks, and only changes direction
when it reaches the inner or outer edge. The head will also change direction if there are 
no more requests for tracks in the direction of travel, but there are requests for tracks
in the opposite direction. The SSTF algorithm will move the IO head to whichever desired
track is closest to its current position.

To help the simulation decide which request to service next, we define a Request class as
was seen in figure \ref{fig:UML}. Not only does the Request object keep a record of all
the important request information like the track number and work to be done, but it also
defines a weight through the less than operator (<). The definition of the less-than
operator depends on the desired scheduling algorithm. A Request object $\mathcal{A}$ is said to be
less than the Request object $\mathcal{B}$ if $\mathcal{A}$ is to be scheduled before
$\mathcal{B}$. The simulated hard drive can then maintain a list of
Request objects as they are made, and decide the next one to service by finding the
smallest Request object. As seen in the UML diagram, the implementation of the less-than
operator is found in a separate file from the rest of the Request definition, to make it
easy to plug in different definitions to get different scheduling algorithms.

\subsection{Hard Drive Simulation Experiment}
In our experiment we first generated a large file of sample input (1000 requests) to be made to
the hard drive. The data consists of tracks chosen from the disk at random, all with an
equal amount of work. This choice was made because this kind of data is easy to generate
and understand. However, by the principle of locality, it's more likely that a
hard disk would receive requests for tracks that are in close proximity.

We generate 5 threads whose purpose is to read requests from the data file, and make the
request call to the monitor. We also generate one servicing thread whose purpose is to
request that the monitor service a pending request. The servicing thread runs
indefinitely, but the requesting threads will terminate once they find there is no more
data in the file.

The flow of execution for the hard disk Monitor calls \emph{DoNextJob} and \emph{Request}
are shown in figure \ref{fig:flow}. These are the functions called by the threads in the
main program. Notice how both functions start and end with the EnterMonitor and
LeaveMonitor calls as described earlier. Also of note are the calls to \emph{signal} and
\emph{wait}, the standard monitor calls. The implementation of the main program logic can
be found in listing \ref{listing:main}.
\begin{figure}[htb!]
    \centering
    \includegraphics[scale=0.8]{300_flow.pdf}
    \label{fig:flow}
    \caption{a flow chart}
\end{figure}

In listings \ref{listing:sample_elevator} and \ref{listing:sample_SSTF}, we show the
first few lines of the output on the experiment, using the Elevator and SSTF algorithms,
respectively. There are three types of lines in the output, those that read \emph{going to
wait}, \emph{Just pushed track X}, and \emph{Working on track X}. These output lines let
us know when the servicing thread is going to wait because there are no pending requests,
when a request is made, and when a request is serviced. By examining the output, we can
see that the scheduling algorithms are working as expected. 

As an example, consider lines 54 to 59 of the output from the SSTF algorithm, in listing
\ref{listing:sample_SSTF}. Previously, track 10 was examined, followed by track 6, before
the servicing thread output \emph{going to wait}. Then starting at line 54, tracks 2 and 9
are requested. Since the IO head moved down from 10 to 6, we know the currect direction is
down. The next lines show that track 9 was scheduled before track 2, because track 9 is
closer to track 6 than is track 2. The Elevator algorithm, on the other hand, would have
scheduled track 2 before track 9, since track 2 was in the direction of travel.

\newpage
\section{Analysis} %Andrew
\subsection{Time to Service a Request}
One of the most important metrics in analyzing the performance of the hard drive is the
time it takes after an IO request is made before it is serviced by the hard drive. This
interval is expected to depend on the number of unserviced requests made to the hard drive
thus far, because the servicing thread always has to scan the entire list of requests in
order to find the next request according to the scheduling rule. The way we ensure that
more requests are pushed into the queue is by increasing the number of requesting threads.

In figures
\ref{fig:waittimeElevator} and \ref{fig:waittimeSSTF}, this delay interval is shown as a 
function of the number of request threads for the Elevator and SSTF scheduling algorithms,
respectively.
This data was gathered from running the simulation multiple times, with increasingly many
request threads, and on input data of 1000 requests generated randomly on the disk.
IO requests from anywhere on the disk. 

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1]{waittimeElevator.pdf}
    \label{fig:waittimeElevator}
    \caption{The wait time per request is improved by using more than one servicing thread.}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1]{waittimeSSTF.pdf}
    \label{fig:waittimeSSTF}
    \caption{The wait time per request is improved by using more than one servicing thread.}
\end{figure}
From the graphs, we can make a few observations. Firstly, we can see that the performance
of both algorithms are comparable using this metric, on this input data. This happens because
the behaviour of the two algorithms are similar when the request queue
contains data from everywhere on the disk. In this situation, both algorithms will scan up
and down the disk in succession, servicing requests along the way. 

Secondly, we can note
the change in performance when increasing number of servicing threads are employed. By
increasing from one to five threads, there is a clear improvement in  performance,
especially when there are many request threads. With only one servicing thread,
there is a divergence from linear behaviour, but this divergence is delayed when using a
large number of servicing threads. However, we can see that when using 100 threads, the
performance is reduced from when using 5 threads. This is caused by the overhead of
the system having to maintain a large number of threads. Therefore, performance by this
metric is optimized by using a small number of servicing threads, but more than one.

%This data was generated by running the simulation with increasingly large number of
%requesting threads, since this causes more requests to get onto the queue before the
%servicing thread can service them. Whenever a request was serviced, the wait time and
%number of requests were recorded, and an average was computed for each number of
%requesting threads. From the figure it's clear that the performance of the two algorithms
%are comparable for this input. This happens because when the request queue becomes filled
%with requests from all over the disk, the Elevator and SSTF algorithms behave more or less
%in the same way, scanning forward and backward through the disk in a cycle.
%
There is a test case in which SSTF performs much worse than the Elevator algorithm. The
SSTF algorithm can suffer from starvation when most of the IO requests are on one end of
the disk (say the outer edge), and then a small number of requests are made for the other
end of the disk (inner edge). If
requests for the outer edge keep coming in before the servicing thread has the chance to
clear the queue of requests, then those requests will never run. The elevator algorithm
does not suffer from this problem, of course, since it scans the disk back and forth.
Unfortunately (or perhaps fortunately!) it is very difficult to construct a test case that demonstrates this
behaviour; the scheduling environment makes it difficult to predict how many
requests will be pending at any time, so it is not possible to predict how long the servicing
thread will execute before pthreads forces it to yield.

\subsection{Distance per Request}
In the on the time to service a request, the analysis assumed that all jobs took the same
amount of time to execute, and so the time was mostly attributable to the scheduling
algorithm, and the relative amount of time spent in requesting threads versus the
servicing thread. In real hardware, the IO head will require some time to move to the
desired track, and so this might be a useful metric. Specifically, we measure the average
number of tracks traversed between the time a request is made, and the same request is
serviced. In figures \ref{fig:distanceElevator} and \ref{fig:distanceSSTF} we show this 
quantity with respect to the number of request threads in the simulation, for the
Elevator and SSTF algorithms, respectively.

\newpage
\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1]{distanceElevator.pdf}
    \label{fig:distanceElevator}
    \caption{The average number of tracks traversed by the IO head per request can depend
    significantly on the number of servicing threads in use.}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1]{distanceSSTF.pdf}
    \label{fig:distanceSSTF}
    \caption{Performans of SSTF under the distance metric is comparable to Elevator on
    random input data.}
\end{figure}
\newpage
Again, it's clear that the two algorithms have similar performance under this metric. This
is again attributable to the behaviour of the two algorithms being similar on the input.
Of particular interest, however, is the profile of the data. When the number of request
threads is fewer than about twenty, the average distance per request increases linearly.
This happens when there are relatively few requests on the queue
(the result of fewer request threads), since the IO head will not travel in a straight line to
the requested track under SSTF or Elevator, but rather will go back and forth between
close requests in SSTF, or in the wrong direction with Elevator. However, when there are
many requests, as explained before, both algorithms will scan the entire disk up and down
servicing all requests. When multiple servicing queues are used, the average distance 
levels off at increasingly
small values. This happens because having more servicing threads reduces the number of
pending requests at any time, and so the average distance travelled does not grow as
much.

Note that regardless of the number of servicing threads, the average number of tracks
traversed begins at around 5. This is the result of our input data being random, and it
can be shown that the expected distance travelled is 5 when using a hard drive of size 15,
as we do in our simulations. When there is only 1 request thread, each request is usually
serviced immediately because the act of requesting when there are no requests pending
signals the servicing thread to do some work. Since the requests are designed to be
random, the average distance ($D$) travelled is the average distance between any two
tracks. This is computed below.
\begin{align*}
\mathrm{E}(D) &= \dsum_{i=0}^N D\times\mathrm{P}(D),\text{expectation formula}\\
              &= \frac{N}{N^2} + \dsum_{i=1}^N (N - i)\times \frac{2i}{N^2},\text{$N$ ways for
                                                      positions to be identical} \\
              &= \frac{1}{N^2}\left[ 2N\cdot\frac{N\cdot(N+1)}{2} 
              - 2\cdot\left(\frac{1}{6}\cdot N \cdot (N+1) \cdot (2N + 1) \right)
              + N \right], \text{using summation formulae}\\
              &= N+1 - \frac{(N+1)\cdot(2N + 1)}{3N} + \frac{1}{N}.
\end{align*}
Plugging in $N = 15$ yields E($D$) = 5.04, which is close to the observed value.
%
%As an example, if there are only two requests
%on the queue, both algorithms will cause the IO head to travel directly to the requested
%track; however, if there are a few more requests, then the Elevator algorithm may travel
%in the opposite direction before coming back, and SSTF might go back and forth serving the
%nearest requests before it gets to the last one. Once the number of requests gets to be large 
%(caused by more request threads), both
%algorithms will service requests linearly on the disk, making a straight path to each
%request, which is why the distance levels off and even decreases slightly with increasing
%number of request threads.
%
%%As a point of interest, in figure \ref{fig:distance100}, we show the same data but now
%%taken when there are 100 servicing threads active. Here we no longer see the levelling off
%%effect, because the servicing threads are more effective at keeping the number of pending
%%requests low.
%This data shows us that there is a peak in distance that we would like to avoid, either by
%using sufficiently few request threads, or sufficiently many. The exact number would need
%to be found experimentally for the expected input.
%
\subsection{Reversals per Request}
Another hardware consideration is the number of times the IO head will need to switch
directions after a request is made and before it is serviced. Just like the distance
metric, in real hardware it is expected that instructing the IO head to reverse direction
would take some time, and it may be a desirable quantity to minimize. Figures
\ref{fig:turnsElevator} and \ref{fig:turnsSSTF} show this quantity with respect to the 
number of request threads, for the Elevator and SSTF algorithms, respectively.
For both algorithms, there is an initial increase in the number of reversals as the number
of request threads increase. This again is attributable to a non-empty request queue which
is sparse. This initial increase is lost when the number of servicing threads is
increased, because the number of pending requests is kept very low.

This is the first metric for which there is a measurable difference in performance. When
there is only one request thread, the SSFT algorithm had runs whose \emph{average} number
of reversals was greater than one. This means that IO head had to reverse direction on
almost every request, and sometimes more than once. This is definitely not very good
behaviour if reverals want to be kept to a minimum. In comparison, the number of reversals
for the Elevator don't vary as much, and the average number of reverals rarely were more
than 0.8.
\newpage
\begin{figure}[htb]
    \centering
    \includegraphics[scale=1]{turnsElevator.pdf}
    \label{fig:turnsElevator}
    \caption{Due to the nature of the Elevator algorithm, the number of turns per request
    is guaranteed never to be more than one.}
\end{figure}

\begin{figure}[htb!]
    \centering
    \includegraphics[scale=1]{turnsSSTF.pdf}
    \label{fig:turnsSSTF}
    \caption{The number of reversals when using SSTF can be more than one per request.}
\end{figure}

%The data shows that the two algorithms perform comparably on this input, and have a
%profile similar to that of the distance metric (although on a smaller scale). The
%explanation for this behaviour is the same as that for the distance metric behaviour: when
%there are few requests on the queue, both algorithms may make a detour 

\newpage
\section{Listings} %will contain trial runs and source code
\subsection{Monitor Interface}
\lstinputlisting[language=C++, label=listing:monitor_h]{Monitor.h}
\subsection{Monitor Implementation}
\lstinputlisting[language=C++, label=listing:monitor_cpp]{Monitor.cpp}
\subsection{Hard Disk Monitor Interface}
\lstinputlisting[language=C++]{HDMonitor.h}
\subsection{Hard Disk Monitor Implementation}
\lstinputlisting[language=C++]{HDMonitor.cpp}
\subsection{Request Class Definition}
\lstinputlisting[language=C++]{Request.h}
\subsection{Scheduling Algorithm Implementations}
The scheduling algorithm is implemented by defining an order on the Request objects in the
HDMonitor class. The order is defined by implementing the less-than operator(<), and this
is done in separate files for each algorithm and linked at compile time.
\subsubsection{Elevator Algorithm}
\lstinputlisting[language=C++]{Elevator.cpp}
\subsubsection{Shortest Seek-Time First Algorithm}
\lstinputlisting[language=C++]{SSTF.cpp}
\subsection{Main Program}
\lstinputlisting[language=C++,label=listing:main]{MonitorDriver.cpp}
\subsection{Sample Output}
\subsubsection{Elevator}
\lstinputlisting[label=listing:sample_elevator]{sample_elevator_output.txt}
\subsubsection{SSTF}
\lstinputlisting[label=listing:sample_SSTF]{sample_SSTF_output.txt}

\tableofcontents %automatically generated
\end{document}
