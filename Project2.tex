\documentclass{report}
\usepackage[vmargin=2.5cm,hmargin=2.5cm]{geometry}
\usepackage[pdftex]{graphicx}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\def\thesection{\arabic{section}}
\begin{document}
\input{./Title.tex}
\section{Introduction}
Monitors are a powerful technique for providing mutual exclusion to shared
resources in a multiprogramming environment, because of their ability to simplify the
programming interface for users of the shared resource. They are, however, a complex
construct that must be supported by the compiler for the language that makes use of them.
Seeing as the design goals of C/C++ are primarily based on speed and efficiency of the
resulting code, Monitors are not supported in these languages. 

In this report we design and implement a software interface that simulates the interface
of a Monitor, using the pthreads package to provide the mutual exclusion required. We then
use the Monitor construct to provide mutual exclusion to a hard drive simulator, which receives IO requests 
from multiple users.

\section{High-level Design} %Joel

\section{Analysis} %Andrew
\subsection{Time to Service a Request}
One of the most important metrics in analyzing the performance of the hard drive is the
time it takes after an IO request is made before it is serviced by the hard drive. This
interval is expected to depend on the number of unserviced requests made to the hard drive
thus far, because the servicing thread always has to scan the entire list of requests in
order to find the next request according to the scheduling rule. In figure
\ref{fig:delay}, this delay interval is shown as a function of the number of unserviced
requests when the request was serviced. This data was gathered from running arbitrary
IO requests from anywhere on the disk. As expected, the data is roughly linear for both
the Elevator and Shortest Seek-Time First algorithms.

This data was generated by running the simulation with increasingly large number of
requesting threads, since this causes more requests to get onto the queue before the
servicing thread can service them. Whenever a request was serviced, the wait time and
number of requests were recorded, and an average was computed for each number of
requesting threads. From the figure it's clear that the performance of the two algorithms
are comparable for this input. This happens because when the request queue becomes filled
with requests from all over the disk, the Elevator and SSTF algorithms behave more or less
in the same way, scanning forward and backward through the disk in a cycle.

There is a test case in which SSTF performs much worse than the Elevator algorithm. The
SSTF algorithm can suffer from starvation when most of the IO requests are on one end of
the disk (say the outer edge), and then a small number of requests are made for the other
end of the disk (inner edge). If
requests for the outer edge keep coming in before the servicing thread has the chance to
clear the queue of requests, then those requests will never run. The elevator algorithm
does not suffer from this problem, of course, since it scans the disk back and forth.
Unfortunately (or perhaps fortunately!) it is very difficult to construct a test case that demonstrates this
behaviour; the scheduling environment makes it difficult to predict how many
requests will be pending at any time, so it is not possible to predict how long the servicing
thread will execute before pthreads forces it to yield.

\section{Listings} %will contain trail runs and source code


\tableofcontents %automatically generated
\end{document}
